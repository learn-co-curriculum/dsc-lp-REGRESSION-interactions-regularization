{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Extensions to Linear Models\n", "\n", "## Interaction Terms and Polynomial Regression"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd \n", "import numpy as np \n", "\n", "import seaborn as sns \n", "%matplotlib inline \n", "\n", "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n", "\n", "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n", "\n", "from sklearn.model_selection import train_test_split \n", "\n", "from sklearn.metrics import mean_squared_error"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["You've seen that when performing multiple linear regression, we are trying to find the best parameters $\\beta_i$ to fit a model of the form \n", "\n", "$$\\large  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$ \n", "\n", "to the data. \n", "\n", "By \"best parameters\", we mean those parameters that minimize the cost function for linear regression.  \n", "\n", "Here $y$ is the dependent variable or target, $n$ is the number of independent variables or features of the model, $\\beta_0$ is the intercept, and $\\beta_i$ (where $i$ is equal to 1, 2, ..., n) is the coefficient that tells us by how much the target $y$ changes if $x_i$ increases by a unit, all other features staying constant. \n", "\n", "By construction, this model assumes that there is a linear relationship between the target variable and the features. \n", "\n", "**But what about times when features have a non-linear relationship to the data, or when features interact with each other?** "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Interactions \n", "Interaction terms allow us to model relationships when the effects of a feature on the target is influenced by another feature. \n", "\n", "$$\\large  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{1, 2} x_1 x_2 + ... + \\beta_n x_n $$ \n", "\n", "### Polynomial terms \n", "\n", "Polynomial terms allow us to capture non-linear relationships between a feature and the target. \n", "\n", "We can use polynomial regression to capture non-linear relationships of features to the target. \n", "\n", "> This is still a linear regression problem because the regression function is linear in the unknown coefficients/parameters $\\beta$ that are estimated from the data. \n", "\n", "$$\\large  y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + ... $$\n", "\n", "**We can use `scikit-learn`'s `PolynomialFeatures` to create polynomial and interaction terms from our features.**\n", "\n", "We will see examples of this tool in action soon below. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, let's discuss one of the most important concepts regarding model fitting: **the bias-variance tradeoff.** "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Bias-Variance Tradeoff"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Bias and Variance\n", "\n", "When fitting a model to data, there are two main sources of error: reducible and irreducible error. \n", "\n", "Reducible error is comprised of bias and variance. \n", "\n", "* **Bias** error is an error from incorrect assumptions in the model. When a model has too high bias it can miss relevant relationships between features and target and can cause a model to underfit and not be able to generalize to unseen data\n", "\n", "* **Variance** is error due to sensitivity to variations in the training set. A model with too high variance trains on the noise rather than the signal, and also doesn't generalize to unseen data and overfits it. \n", "\n", "**Bias error measures how much the predicted values of your model differ from the true values.** Bias occurs when a model has limited flexibility to learn the _true_ signal from a dataset. \n", "\n", "**Variance error occurs when a model is very sensitive to specific sets of training data.** \n", "\n", "We can decompose the reducible error into bias and variance.\n", "\n", "<img src=\"images/bias-variance-tradeoff.png\" width=500>\n", "\n", "(source: [The Bias-Variance Tradeoff](https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If your model has too few predictors or if you have too few data in your training set, it might suffer from **high bias** and underfit your data. \n", "\n", "If your model is too complex and has too many predictors, it might suffer from **high variance** and overfit your data. \n", "\n", "In both cases, the model won't generalize well to unseen data!  \n", "\n", "We want to train models with _just enough_ number of features to minimize the model's generalization error.\n", "\n", "One of the ways we can control model complexity is by using **Regularization**, which we discuss next. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Regularization"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Suppose I have split my data into training and testing sets. Do I want my model to fit my training data _exactly_ ?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["> No. A model like this will have too much variance (fitting the noise!) and overfit the data. It won't generalize to the unseen test data. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Overfitting is generally a result of high variance. \n", "\n", "High variance can be caused by:\n", "    \n", "- having irrelevant or too many predictors\n", "\n", "- multicollinearity\n", "\n", "- large coefficients\n", "\n", "The first problem is about picking up on noise rather than signal.\n", "\n", "The second problem is about having a least-squares estimate that is highly sensitive to random error.\n", "\n", "The third is about having highly sensitive predictors. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Regularization to the rescue! \n", "\n", "**Regularization** is about introducing a factor into our model's cost function designed to enforce the stricture that the coefficients stay small, by **penalizing** the coefficients that get too large.\n", "\n", "This technique helps us avoid model overfitting, as it discourages \"learning\" a very complex model (by shrinking coefficients to smaller values or shrinking them to zero altogether!) \n", "\n", "To implement regularization techniques, we alter the linear regression __cost function__ such that the goal now is not merely to minimize the difference between actual values and our model's predicted values. \n", "\n", "* Rather, we add in a term to our loss function that represents the sizes of the coefficients."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Lasso (\"L1\") Regularization: \n", "\n", "Modified cost function: \n", "\n", "$$\\large\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j \\times x_{ij})^2]+ \\lambda\\space\\Sigma^{n_{feat.}}_{j=0}|\\beta_j|$$\n", "<br/> <br/>\n", "\n", "### Ridge (\"L2\") Regularization: \n", "\n", "Modified cost function: \n", "\n", "$$\\large\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j \\times x_{ij})^2] + \\lambda\\space\\Sigma^{n_{feat.}}_{j=0}\\beta^2_j$$\n", "\n", "\n", "**Don't let these formulas intimidate you!** \n", "\n", "The first term in each of these (the sum of squares) is the same, familiar cost function (also known as loss function) we use in linear regression! \n", "\n", "What distinguishes Lasso Regression from Ridge Regression is the additional term on the right: the penalty term!  \n", "\n", "* The Lasso uses the absolute values of the coefficients.\n", "    * As a consequence of the form of this penalty term, the Lasso can set the coefficient for a feature equal to zero. \n", "        * This is why the Lasso is used as a feature selection method. \n", "<br> </br>\n", "* The Ridge uses the squares of the coefficients. \n", "\n", "For a nice discussion of these methods in Python, see https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Scale the features before using Regularization!\n", "\n", "When using regularization methods, we need to scale the features first! \n", "\n", "The terms added to the cost function in both Lasso and Ridge regression penalize large values of all coefficients **equally**. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Regularization: Example: Boston Housing Dataset\n", "\n", "We'll use Lasso regression to fit a model to the Boston Housing Data using scikit-learn. \n", "\n", "In the cell below, we load the data for you and fit a baseline linear regression model to compare our Lasso regression model results to once we have them."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.datasets import load_boston\n", "\n", "X, y = load_boston(return_X_y=True)\n", "\n", "boston = load_boston()\n", "\n", "df = pd.DataFrame(np.concatenate([X,y.reshape(-1,1)], axis=1), columns=boston.feature_names.tolist() + ['MEDV'])\n", "df.head(1)\n", "\n", "X = df.drop('MEDV', axis=1)\n", "y = df['MEDV']\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n", "\n", "\n", "# Train a baseline linenar regression model to the data\n", "lr = LinearRegression()\n", "lr.fit(X_train, y_train)\n", "\n", "print('Training score: {}'.format(lr.score(X_train, y_train)))\n", "print('Test score: {}'.format(lr.score(X_test, y_test)))\n", "\n", "y_pred = lr.predict(X_train)\n", "mse = mean_squared_error(y_train, y_pred)\n", "rmse = np.sqrt(mse)\n", "print('Training MSE: {}'.format(mse))\n", "\n", "y_pred = lr.predict(X_test)\n", "mse = mean_squared_error(y_test, y_pred)\n", "\n", "print('Test MSE: {}'.format(mse))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's first attempt to improve upon this model by adding polynomial features and interaction terms. \n", "\n", "We create the polynomial and interaction terms and fit the model for you. \n", "\n", "Evaluate the model performance on the test data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["poly = PolynomialFeatures(degree=2, include_bias=False)\n", "X_train_poly = poly.fit_transform(X_train)\n", "\n", "lr2 = LinearRegression()\n", "lr2.fit(X_train_poly, y_train)\n", "print(\"Training score:\", lr2.score(X_train_poly, y_train))\n", "\n", "mse = mean_squared_error(y_train, lr2.predict(X_train_poly))\n", "print(\"Training MSE:\", mse)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Transform the test data \n", "X_test_poly = poly.transform(None)\n", "\n", "# Print R-squared for the test data \n", "print(\"Test score:\", lr2.score(None, None))\n", "\n", "# Compute and print the mean squared error of the model on the test data \n", "mse = mean_squared_error(y_train, lr2.predict(None))\n", "\n", "print('Test MSE: {}'.format(mse))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How does the performance of the trained model compare on the training vs test data? **Compare the two resulting mean squared errors.**"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's use Lasso regularization using the polynomial features to see if we can fit a model that outperforms the baseline linear regression model we fit previously."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Scale the features first!\n", "\n", "scaler = StandardScaler()\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)\n", "\n", "# Create polynomial and interaction terms \n", "poly = PolynomialFeatures(degree=2, include_bias=False)\n", "X_train_scaled_poly = poly.fit_transform(X_train_scaled)\n", "X_test_scaled_poly = poly.transform(X_test_scaled)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Instantiate and fit a lasso model. Set alpha = 0.3 and random_state=1\n", "lasso = Lasso(alpha=None, random_state=None)\n", "lasso.fit(None, None)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Compute and print the R-squared of the model on the training data\n", "print(\"Training score:\", lasso.score(None, None))\n", "\n", "# Compute and print the mean squared error of the model on the test data \n", "mse = mean_squared_error(y_train, lasso.predict(None))\n", "\n", "print('Training MSE: {}'.format(mse))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Compute and print the R-squared of the model on the training data\n", "print(\"Training score:\", lasso.score(X_train_scaled_poly, y_train))\n", "\n", "# Compute and print the mean squared error of the model on the test data \n", "mse = mean_squared_error(y_train, lasso.predict(X_train_scaled_poly))\n", "\n", "print('Training MSE: {}'.format(mse))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Compute and print the R-squared of the model on the training data\n", "print(\"Test score:\", lasso.score(None, None))\n", "\n", "# Compute and print the mean squared error of the model on the test data \n", "mse = mean_squared_error(None, lasso.predict(None))\n", "\n", "print('Test MSE: {}'.format(mse))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Compare the performance of this model on the test data with the performance of the baseline model trained at the beginning. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Did the Lasso set any coefficients to zero? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Summary: \n", "\n", "1. Interaction and polynomial terms can be added to linear models to account for non-linear relationships between the features and target, and to account for effects of a feature on the target when it is influenced by another feature. \n", "\n", "2. You can use `sklearn.preprocessing.PolynomialFeatures` to create interaction terms and polynomial terms. \n", "\n", "3. Error due to bias may occur when we use too few features in our model or when we don't have enough data. Errors due to variance occur when we fit a model that is too complex that overfits to the noise of the data and fails to generalize to unseen data.\n", "\n", "4. We use regularization techniques like the Lasso and Ridge regression to reign in overfitting models by reducing their complexity. \n", "\n", "5. Before applying regularization techniques, you **must** scale the features. \n", "\n", "6. You can implement Lasso regularization in Python using `sklearn.linear_model.Lasso`. \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Optional\n", "\n", "## Example: Interaction and polynomial terms"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The `Advertising` dataset has data on the number of sales of a product in 200 different markets, with information about how much money was spent on advertising on TV, radio, and on the newspaper. \n", "\n", "We want to know how advertising on these different media effects number of sales of the product. \n", "\n", "In the cell below, we load the `Advertising` dataset for you. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df = pd.read_csv('Advertising.csv')\n", "\n", "# Look at the first five rows of data\n", "df.head(1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Let's drop the first column; it's meaningless\n", "df.drop('Unnamed: 0', axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We will fit a baseline linear regression model to the data. \n", "\n", "* We first make sure to split the data into training and test sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X = df[['TV', 'radio', 'newspaper']]\n", "y = df['sales']\n", "\n", "# Split the data into training and test sets. \n", "# Use random_state = 1 for reproducibility of results. \n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["* Next, we instantiate a `LinearRegression` model and fit the model to the training data "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["lr = LinearRegression()\n", "lr.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, we want to assess model performance. \n", "\n", "To start, we compute the model's coefficient of determination, $R^2$ on the training and test data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["r2 = lr.score(X_train, y_train)\n", "print(\"Training set R-squared:\", r2)\n", "\n", "r2 = lr.score(X_test, y_test)\n", "print(\"Test set R-squared:\", r2)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, we look at the model's adjusted coefficient of determination, $R_{adj}^{2}$ on the training and test data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def adjusted_r2(X, y, fitted_model):\n", "    r2 = fitted_model.score(X, y)\n", "    n = X.shape[0]\n", "    k = X.shape[1]\n", "    adj_r2 = 1 - ((1-r2)*(n-1))/(n - k - 1)\n", "    return adj_r2\n", "\n", "print(\"Training set Adjusted R-squared\", adjusted_r2(X_train, y_train, lr))\n", "print(\"Test set Adjusted R-squared\", adjusted_r2(X_test, y_test, lr))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Finally, we look at the model's mean squared error on the training and test data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["mse_training = mean_squared_error(y_train, lr.predict(X_train))\n", "print(\"Training set MSE:\", mse_training)\n", "\n", "mse_test = mean_squared_error(y_test, lr.predict(X_test))\n", "print(\"Test set MSE:\", mse_test)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["One way we can attempt to improve on this model is to include interaction terms. \n", "\n", "Maybe spending the advertising budget on one media, like newspaper, increases the effectiveness of TV advertising. In marketing, they call this synergy effects. We call this interactions. \n", "\n", "Let's add this interaction term to our data and fit a linear regression model that takes this into account."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add interaction term between TV and newspaper\n", "df['TV*newspaper'] = df['TV']*df['newspaper']\n", "\n", "# Inspect the data\n", "df.head(1)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Fit a linear regression model to predict sales using the `TV`, `radio`, `newspaper`, and `TV*newspaper` features.** \n", "\n", "_Hint: Follow the steps that we used above to fit the linear regression model_"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Split data into training and test sets. Use random_state=1. \n", "\n", "# your code here \n", "\n", "# Instantiate a LinearRegression object and fit it to the training data \n", "\n", "# your code here \n", "\n", "# Compute the trained model's performance on the training and test data:\n", "\n", "# R2 \n", "r2 = None\n", "print(\"Training set R-squared:\", r2)\n", "\n", "r2 = None \n", "print(\"Test set R-squared:\", r2)\n", "\n", "# Adjusted R2 \n", "print(\"Training set Adjusted R-squared\", None)\n", "print(\"Test set Adjusted R-squared\", None)\n", "\n", "# Mean Squared Error \n", "mse_training = None \n", "print(\"Training set MSE:\", mse_training)\n", "\n", "mse_test = None \n", "print(\"Test set MSE:\", mse_test)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Did the model performance change?** "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's take a quick look at the original data using `sns.pairplot`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["sns.pairplot(df.drop(['TV*newspaper'], axis=1))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What if we wanted to know about other possible interactions between the features and their effect on `sales`?\n", "\n", "Also, from our pair plot above, we see that the relation between the features and the target is not exactly linear. \n", "\n", "We can use `sklearn.preprocessing.PolynomialFeatures` to generate polynomial and interaction features. \n", "\n", "Let's see what fitting a model with all these features does for our model performance! "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["To create the polynomial and interaction terms, run the cell below. \n", "\n", "We are only interested in creating terms with degree 2. This means we care about creating features like the square of the TV advertising budget or interaction terms like TV budget times newspaper budget. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["poly = PolynomialFeatures(degree=2, include_bias=False)\n", "X_poly = poly.fit_transform(df[['TV','radio','newspaper']])\n", "X_poly = pd.DataFrame(X_poly, columns=poly.get_feature_names(['TV','radio','newspaper']))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, you'll fit a linear regression model and assess model performance on the training and test data. \n", "\n", "You know the drill! (See the examples above.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Split data into training and test sets. Use random_state=1. \n", "\n", "# your code here \n", "\n", "# Instantiate a LinearRegression object and fit it to the training data \n", "\n", "# your code here \n", "\n", "# Compute the trained model's performance on the training and test data:\n", "\n", "# R2 \n", "r2 = None\n", "print(\"Training set R-squared:\", r2)\n", "\n", "r2 = None \n", "print(\"Test set R-squared:\", r2)\n", "\n", "# Adjusted R2 \n", "print(\"Training set Adjusted R-squared\", None)\n", "print(\"Test set Adjusted R-squared\", None)\n", "\n", "# Mean Squared Error \n", "mse_training = None \n", "print(\"Training set MSE:\", mse_training)\n", "\n", "mse_test = None \n", "print(\"Test set MSE:\", mse_test)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Did the model performance change?**"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Ridge regression "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Repeat the Boston housing dataset, this time using ridge regression instead of the Lasso. "]}], "metadata": {"celltoolbar": "Slideshow", "kernelspec": {"display_name": "learn-env", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}